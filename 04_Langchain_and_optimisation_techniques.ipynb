{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwJyH3/XD7om0G9bLCcfB1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/od41/handson-lm/blob/main/04_Langchain_and_optimisation_techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "-10lMJB1fayj",
        "outputId": "df0ab0a3-aa63-4d9f-e506-c1d3c25565a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf"
      ],
      "metadata": {
        "id": "t1WSDxgSUcT5",
        "outputId": "17c9dc18-b579-4c4d-c3f7-a68fbf74808e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-08 00:55:34--  https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.238.49.112, 18.238.49.10, 18.238.49.117, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.238.49.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/8a83c7fb9049a9b2e92266fa7ad04933bb53aa1e85136b7b30f1b8000ff2edef?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-q4.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-q4.gguf%22%3B&Expires=1741398934&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MTM5ODkzNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvOGE4M2M3ZmI5MDQ5YTliMmU5MjI2NmZhN2FkMDQ5MzNiYjUzYWExZTg1MTM2YjdiMzBmMWI4MDAwZmYyZWRlZj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=eUiNLNJ7jnOGlFMMYJbWSwQ7iG2Qz07qwXYj0%7EjLoYniqK2bM-hVrIowF2DcYaUzKn9M3vKP-%7EY7PtoCCd176M7cYTHmpe8mgkR9d1dE3YbAkIdbDND8M-Byr5JXu4ff-sODhbxqpURtZzY6hPewi2E59HhVNtflvvOtaIsWWMzKTRkcHr3yZZjklmzchsdlMIIURDuSe3DY-hdyx0qQUnzMgDliLWbF8ps%7EviVBIYAUlDCQYY3%7E74VfH6%7EOOUc3b9mkQD1FF9SxbW6AcW3OJx4giZboscqwq5g0o0wMcFjk5JHlFPedhvr6SklzCj7ome0ocxdAQIz%7EXHKGrZXlNw__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-03-08 00:55:34--  https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/8a83c7fb9049a9b2e92266fa7ad04933bb53aa1e85136b7b30f1b8000ff2edef?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-q4.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-q4.gguf%22%3B&Expires=1741398934&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MTM5ODkzNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvOGE4M2M3ZmI5MDQ5YTliMmU5MjI2NmZhN2FkMDQ5MzNiYjUzYWExZTg1MTM2YjdiMzBmMWI4MDAwZmYyZWRlZj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=eUiNLNJ7jnOGlFMMYJbWSwQ7iG2Qz07qwXYj0%7EjLoYniqK2bM-hVrIowF2DcYaUzKn9M3vKP-%7EY7PtoCCd176M7cYTHmpe8mgkR9d1dE3YbAkIdbDND8M-Byr5JXu4ff-sODhbxqpURtZzY6hPewi2E59HhVNtflvvOtaIsWWMzKTRkcHr3yZZjklmzchsdlMIIURDuSe3DY-hdyx0qQUnzMgDliLWbF8ps%7EviVBIYAUlDCQYY3%7E74VfH6%7EOOUc3b9mkQD1FF9SxbW6AcW3OJx4giZboscqwq5g0o0wMcFjk5JHlFPedhvr6SklzCj7ome0ocxdAQIz%7EXHKGrZXlNw__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 52.85.61.80, 52.85.61.116, 52.85.61.64, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|52.85.61.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2393231072 (2.2G) [binary/octet-stream]\n",
            "Saving to: ‘Phi-3-mini-4k-instruct-q4.gguf.1’\n",
            "\n",
            "Phi-3-mini-4k-instr 100%[===================>]   2.23G  21.9MB/s    in 56s     \n",
            "\n",
            "2025-03-08 00:56:30 (40.8 MB/s) - ‘Phi-3-mini-4k-instruct-q4.gguf.1’ saved [2393231072/2393231072]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3b6FTYympHRh",
        "collapsed": true,
        "outputId": "e3449df6-ac7d-4642-be6b-5776dd0a134d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.20)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.43)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.8.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# command to install on a CPU\n",
        "%pip install --upgrade --quiet  llama-cpp-python\n",
        "\n",
        "# command to install on a GPU\n",
        "# !CMAKE_ARGS=\"-DGGML_CUDA=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "TpBfJklpQ-6v",
        "outputId": "3a4e5539-22bf-4593-962c-ca8a965443c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain import PromptTemplate\n",
        "# <|user|>\\n{prompt}<|end|>\\n<|assistant|>\n",
        "template = \"\"\"<|user|>\n",
        " {input_prompt}\n",
        " <|end|>\n",
        " <|assistant|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "  model_path=\"/content/Phi-3-mini-4k-instruct-q4.gguf\",\n",
        "  # n_gpu_layers=-1,\n",
        "  stop=[\"<|end|>\"],\n",
        "  max_tokens=1000,\n",
        "  n_ctx=2048,\n",
        "  seed=42,\n",
        ")\n",
        "\n",
        "basic_chain = prompt | llm\n",
        "\n",
        "question = \"\"\"\"\n",
        "\"input_prompt\": \"Create a sci-fi story summary\",\n",
        "\"\"\"\n",
        "basic_chain.invoke(question)"
      ],
      "metadata": {
        "id": "tke-KtYmKjrp",
        "outputId": "6fbd67cc-e735-4c3f-a003-8518ba644d4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 24 key-value pairs and 195 tensors from /content/Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
            "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
            "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:   81 tensors\n",
            "llama_model_loader: - type q5_K:   32 tensors\n",
            "llama_model_loader: - type q6_K:   17 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 2.23 GiB (5.01 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control-looking token:  32007 '<|end|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
            "load: control-looking token:  32000 '<|endoftext|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: special tokens cache size = 67\n",
            "load: token to piece cache size = 0.1690 MB\n",
            "print_info: arch             = phi3\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 4096\n",
            "print_info: n_embd           = 3072\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 32\n",
            "print_info: n_rot            = 96\n",
            "print_info: n_swa            = 2047\n",
            "print_info: n_embd_head_k    = 96\n",
            "print_info: n_embd_head_v    = 96\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 3072\n",
            "print_info: n_embd_v_gqa     = 3072\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: n_ff             = 8192\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 4096\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 3B\n",
            "print_info: model params     = 3.82 B\n",
            "print_info: general.name     = Phi3\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32064\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 32000 '<|endoftext|>'\n",
            "print_info: EOT token        = 32007 '<|end|>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: PAD token        = 32000 '<|endoftext|>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 32000 '<|endoftext|>'\n",
            "print_info: EOG token        = 32007 '<|end|>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: layer   0 assigned to device CPU\n",
            "load_tensors: layer   1 assigned to device CPU\n",
            "load_tensors: layer   2 assigned to device CPU\n",
            "load_tensors: layer   3 assigned to device CPU\n",
            "load_tensors: layer   4 assigned to device CPU\n",
            "load_tensors: layer   5 assigned to device CPU\n",
            "load_tensors: layer   6 assigned to device CPU\n",
            "load_tensors: layer   7 assigned to device CPU\n",
            "load_tensors: layer   8 assigned to device CPU\n",
            "load_tensors: layer   9 assigned to device CPU\n",
            "load_tensors: layer  10 assigned to device CPU\n",
            "load_tensors: layer  11 assigned to device CPU\n",
            "load_tensors: layer  12 assigned to device CPU\n",
            "load_tensors: layer  13 assigned to device CPU\n",
            "load_tensors: layer  14 assigned to device CPU\n",
            "load_tensors: layer  15 assigned to device CPU\n",
            "load_tensors: layer  16 assigned to device CPU\n",
            "load_tensors: layer  17 assigned to device CPU\n",
            "load_tensors: layer  18 assigned to device CPU\n",
            "load_tensors: layer  19 assigned to device CPU\n",
            "load_tensors: layer  20 assigned to device CPU\n",
            "load_tensors: layer  21 assigned to device CPU\n",
            "load_tensors: layer  22 assigned to device CPU\n",
            "load_tensors: layer  23 assigned to device CPU\n",
            "load_tensors: layer  24 assigned to device CPU\n",
            "load_tensors: layer  25 assigned to device CPU\n",
            "load_tensors: layer  26 assigned to device CPU\n",
            "load_tensors: layer  27 assigned to device CPU\n",
            "load_tensors: layer  28 assigned to device CPU\n",
            "load_tensors: layer  29 assigned to device CPU\n",
            "load_tensors: layer  30 assigned to device CPU\n",
            "load_tensors: layer  31 assigned to device CPU\n",
            "load_tensors: layer  32 assigned to device CPU\n",
            "load_tensors: tensor 'token_embd.weight' (q4_K) (and 194 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =  2281.66 MiB\n",
            "llama_init_from_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 2048\n",
            "llama_init_from_model: n_ctx_per_seq = 2048\n",
            "llama_init_from_model: n_batch       = 32\n",
            "llama_init_from_model: n_ubatch      = 8\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 10000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 30: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init: layer 31: n_embd_k_gqa = 3072, n_embd_v_gqa = 3072\n",
            "llama_kv_cache_init:        CPU KV buffer size =   768.00 MiB\n",
            "llama_init_from_model: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =     2.81 MiB\n",
            "llama_init_from_model: graph nodes  = 1286\n",
            "llama_init_from_model: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.bos_token_id': '1', 'general.architecture': 'phi3', 'phi3.context_length': '4096', 'phi3.attention.head_count_kv': '32', 'general.name': 'Phi3', 'tokenizer.ggml.pre': 'default', 'phi3.embedding_length': '3072', 'tokenizer.ggml.unknown_token_id': '0', 'phi3.feed_forward_length': '8192', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.block_count': '32', 'phi3.attention.head_count': '32', 'phi3.rope.dimension_count': '96', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
            "' + message['content'] + '<|end|>' + '\n",
            "' + '<|assistant|>' + '\n",
            "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
            "'}}{% endif %}{% endfor %}\n",
            "Using chat eos_token: <|endoftext|>\n",
            "Using chat bos_token: <s>\n",
            "llama_perf_context_print:        load time =    7113.65 ms\n",
            "llama_perf_context_print: prompt eval time =    7113.51 ms /    24 tokens (  296.40 ms per token,     3.37 tokens per second)\n",
            "llama_perf_context_print:        eval time =  216652.36 ms /   522 runs   (  415.04 ms per token,     2.41 tokens per second)\n",
            "llama_perf_context_print:       total time =  224674.37 ms /   546 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Title: Echoes of the Lost Nebula\\n\\nIn the year 2450, humanity has ventured far beyond the confines of Earth and colonized numerous planets within the Andromeda galaxy. However, a long-forgotten ancient alien civilization known as The Luminae left behind an enigmatic nebula that holds unimaginable powers and secrets.\\n\\nAs Earth\\'s largest interstellar corporation, Helix Infinity Systems (HIS) leads the expedition to explore this mysterious Nebula of Echoes. A team comprised of renowned scientists, engineers, astronauts, and a highly-skilled special forces operative named Commander Eli Vega is assembled for the mission: unraveling the secrets held within the nebula while protecting HIS interests from rival corporations seeking to exploit its power.\\n\\nHowever, as they venture deeper into the Echo Nebula, strange occurrences begin plaguing their crew and technology. The team discovers that certain individuals can tap into psychic abilities once exposed to radiation within the nebula. They soon learn of an ancient prophecy foretelling a chosen few who will use these newfound powers as both saviors and destroyers, depending on whether they remain loyal to HIS or succumb to temptation.\\n\\nAs trust between crew members grows thin and factions form, the team must confront not only their own ambitions but also external threats from rogue AI systems created by an elusive group of Luminae survivors who seek to reclaim what was once theirs - a plan that threatens to plunge the entire galaxy into chaos.\\n\\nEli Vega, torn between his sense of duty and loyalty toward HIS, must now decide if he will protect the secrets within Echo Nebula or risk everything for redemption. As Eli grapples with this difficult choice, the fate of humanity rests upon their shoulders while ancient Luminae forces rise to reclaim what was lost - echoing through the vast expanse of Andromeda.\\n\\nThroughout \"Echoes of the Lost Nebula,\" players will experience a thrilling sci-fi adventure filled with psychic battles, intense spacecraft combat, and strategizing alongside high-stakes choices that impact both HIS interests and the destiny of humanity itself.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMChain\n",
        "\n",
        "# Create a chain for the title of our story\n",
        "template = \"\"\"<|user|>\n",
        "Create a title for a story about {summary}. Only return the title.\n",
        "<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "title_prompt = PromptTemplate(template=template, input_variables=[\"summary\"])\n",
        "title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")\n",
        "\n",
        "title.invoke({\"summary\": \"\"\"In the year 2450, humanity has ventured far beyond the confines of Earth and colonized numerous planets within the Andromeda galaxy. However, a long-forgotten ancient alien civilization known as The Luminae left behind an enigmatic nebula that holds unimaginable powers and secrets.\\n\\nAs Earth\\'s largest interstellar corporation, Helix Infinity Systems (HIS) leads the expedition to explore this mysterious Nebula of Echoes. A team comprised of renowned scientists, engineers, astronauts, and a highly-skilled special forces operative named Commander Eli Vega is assembled for the mission: unraveling the secrets held within the nebula while protecting HIS interests from rival corporations seeking to exploit its power.\\n\\nHowever, as they venture deeper into the Echo Nebula, strange occurrences begin plaguing their crew and technology. The team discovers that certain individuals can tap into psychic abilities once exposed to radiation within the nebula.\"\"\"})"
      ],
      "metadata": {
        "id": "nJVmdIYVSyXr",
        "outputId": "fb1a4d19-06a0-411b-8911-cae88fa5f641",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-f662d52b980f>:9: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")\n",
            "Llama.generate: 2 prefix-match hit, remaining 250 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =    7113.65 ms\n",
            "llama_perf_context_print: prompt eval time =   67512.82 ms /   250 tokens (  270.05 ms per token,     3.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8479.92 ms /    20 runs   (  424.00 ms per token,     2.36 tokens per second)\n",
            "llama_perf_context_print:       total time =   76020.38 ms /   270 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': \"In the year 2450, humanity has ventured far beyond the confines of Earth and colonized numerous planets within the Andromeda galaxy. However, a long-forgotten ancient alien civilization known as The Luminae left behind an enigmatic nebula that holds unimaginable powers and secrets.\\n\\nAs Earth's largest interstellar corporation, Helix Infinity Systems (HIS) leads the expedition to explore this mysterious Nebula of Echoes. A team comprised of renowned scientists, engineers, astronauts, and a highly-skilled special forces operative named Commander Eli Vega is assembled for the mission: unraveling the secrets held within the nebula while protecting HIS interests from rival corporations seeking to exploit its power.\\n\\nHowever, as they venture deeper into the Echo Nebula, strange occurrences begin plaguing their crew and technology. The team discovers that certain individuals can tap into psychic abilities once exposed to radiation within the nebula.\",\n",
              " 'title': ' \"Echoes of the Luminae: The Cosmic Conspiracy Unveiled\"'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a chain for the character description using the summary and title\n",
        "template = \"\"\"<s><|user|>\n",
        "Describe the main character of a story about {summary} with the title\n",
        "{title}. Use only two sentences.\n",
        "<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "character_prompt = PromptTemplate(template=template, input_variables=[\"summary\", \"title\"]\n",
        ")\n",
        "character = LLMChain(llm=llm, prompt=character_prompt, output_key=\"character\")"
      ],
      "metadata": {
        "id": "nnwh0mjQUkUf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}